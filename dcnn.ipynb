{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.backend as K\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import (Activation, Add, AveragePooling2D,\n",
    "                          BatchNormalization, Conv2D, Dense, Dropout, Flatten,\n",
    "                          GlobalAveragePooling2D, MaxPool2D, MaxPooling2D,\n",
    "                          ZeroPadding2D, concatenate)\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops.state_ops import scatter_nd_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 8\n",
    "INPUT_SHAPE = (75, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(self, ):\n",
    "    return pd.read_csv('./dcnn-dataset/skin-lesion/HAM10000_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.path = 'model/dcnn.h5'\n",
    "self.check_model_exist()\n",
    "self.mean = 159.8174582437455\n",
    "self.std = 46.41408053415013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNN_Model:\n",
    "    def __init__(self):\n",
    "        print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "        print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "        self.path = 'model/dcnn.h5'\n",
    "        self.check_model_exist()\n",
    "        self.mean = 159.8174582437455\n",
    "        self.std = 46.41408053415013\n",
    "\n",
    "    def check_model_exist(self):\n",
    "        if os.path.exists(self.path):\n",
    "            # If Exist Load Model\n",
    "            print('Load Model')\n",
    "            self.load_model()\n",
    "        else:\n",
    "            # If it doesn't Exit Build Model\n",
    "            print('Making Model')\n",
    "            self.main()\n",
    "\n",
    "    # ----------------\n",
    "    # MAKE MODEL\n",
    "    # ----------------\n",
    "\n",
    "    def read_data(self, ):\n",
    "        return pd.read_csv('./dcnn-dataset/skin-lesion/HAM10000_metadata.csv')\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        # Preprocess Data\n",
    "        paths = {}\n",
    "\n",
    "        # Get a dictionary filled with all image Ids, and their path\n",
    "        for x in glob(os.path.join(\"./dcnn-dataset/skin-lesion/\", \"*\", \"*.jpg\")):\n",
    "            paths[os.path.splitext(os.path.basename(x))[0]] = x\n",
    "\n",
    "        df['path'] = df['image_id'].map(paths.get)\n",
    "\n",
    "        # Get the legible lesion type from the symbol\n",
    "        lesion_type = {\n",
    "            'nv': 'Melanocytic nevi',\n",
    "            'mel': 'Melanoma',\n",
    "            'bkl': 'Benign keratosis-like lesions ',\n",
    "            'bcc': 'Basal cell carcinoma',\n",
    "            'akiec': 'Actinic keratoses',\n",
    "            'vasc': 'Vascular lesions',\n",
    "            'df': 'Dermatofibroma',\n",
    "            \"clr\": \"Clear Skin\"\n",
    "        }\n",
    "\n",
    "        df['cell_type'] = df['dx'].map(lesion_type.get)\n",
    "        df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes\n",
    "        df['image'] = df['path'].map(lambda image: np.asarray(Image.open(image).resize((100, 75))))\n",
    "        df = self.load_clear_skin_datasets(df)\n",
    "        return df\n",
    "\n",
    "    def smote_sampling(self, features, target):\n",
    "        rus = SMOTE(random_state=42)\n",
    "        features, target = rus.fit_resample(features.reshape(-1, INPUT_SHAPE[0] * INPUT_SHAPE[1] * INPUT_SHAPE[2]),\n",
    "                                            target)\n",
    "        features = features.reshape(-1, *INPUT_SHAPE)\n",
    "        return features, target\n",
    "\n",
    "    def image_preprocessing(self, features, target):\n",
    "        features = np.asarray(features.tolist())\n",
    "        feature_mean = np.mean(features)\n",
    "        feature_std = np.std(features)\n",
    "\n",
    "        print(\"Mean: \", feature_mean)\n",
    "        print(\"Std: \", feature_std)\n",
    "\n",
    "        features = (features - feature_mean) / feature_std\n",
    "\n",
    "        # Blue starts here\n",
    "        # plt.imshow((features[0]).astype('uint8'))\n",
    "        # plt.title(\"Normalized Image\")\n",
    "        # plt.show()\n",
    "\n",
    "        # One Hot Encoder\n",
    "        target = to_categorical(target, num_classes=NUM_CLASSES)\n",
    "\n",
    "        # Reshape image to 3 dimension (75 * 100 * 3)\n",
    "        features = features.reshape(features.shape[0], *INPUT_SHAPE)\n",
    "        return self.smote_sampling(features, target)\n",
    "\n",
    "    def load_clear_skin_datasets(self, df):\n",
    "        print(\"Loading Normal Clear Skin Dataset\")\n",
    "\n",
    "        clear_skin_image_list = []\n",
    "\n",
    "        for filename in os.listdir(\"./dcnn-dataset/normal-skin\"):\n",
    "            path = \"./dcnn-dataset/normal-skin/\" + filename\n",
    "            clear_skin_image_list.append(np.asarray(Image.open(path).resize((100, 75))))\n",
    "\n",
    "        image_series = pd.Series(clear_skin_image_list, name='image')\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame({\"image\": image_series, 'cell_type_idx': 7})], ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def convolutional_block(self, x, growth_rate, dropout_rate=None):\n",
    "        x1 = BatchNormalization()(x)\n",
    "        x1 = Activation('relu')(x1)\n",
    "        x1 = Conv2D(growth_rate * 4, (1, 1), padding='same')(x1)\n",
    "\n",
    "        if dropout_rate:\n",
    "            x1 = Dropout(dropout_rate)(x1)\n",
    "\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = Activation('relu')(x1)\n",
    "        x1 = ZeroPadding2D((1, 1))(x1)\n",
    "        x = ZeroPadding2D((1, 1))(x)\n",
    "        x1 = Conv2D(growth_rate, (3, 3), padding='same')(x1)\n",
    "\n",
    "        if dropout_rate:\n",
    "            x1 = Dropout(dropout_rate)(x1)\n",
    "        x = concatenate([x, x1])\n",
    "        return x\n",
    "\n",
    "    def transition_block(self, x, reduction):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(int(x.shape[-1] * reduction), (1, 1), padding='same')(x)\n",
    "        x = AveragePooling2D((2, 2), padding='same', strides=2)(x)\n",
    "        return x\n",
    "\n",
    "    def dense_block(self, x, growth_rate, n_layers):\n",
    "        for i in range(n_layers):\n",
    "            x = self.convolutional_block(x, growth_rate)\n",
    "        return x\n",
    "\n",
    "    # def DenseNet(self, growth_rate=32):\n",
    "    #     # Laptop ga kuat\n",
    "    #     x_input = Input(INPUT_SHAPE)\n",
    "    #\n",
    "    #     x = Conv2D(growth_rate * 2, (7, 7), padding='same', strides=(2, 2))(x_input)\n",
    "    #     x = BatchNormalization()(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "    #     x = AveragePooling2D((3, 3), padding='same', strides=2)(x)\n",
    "    #\n",
    "    #     x = dense_block(x, growth_rate, 4)\n",
    "    #     x = transition_block(x, 0.5)\n",
    "    #\n",
    "    #     x = dense_block(x, growth_rate, 8)\n",
    "    #     x = transition_block(x, 0.5)\n",
    "    #\n",
    "    #     x = dense_block(x, growth_rate, 16)\n",
    "    #     x = transition_block(x, 0.5)\n",
    "    #\n",
    "    #     x = GlobalAveragePooling2D()(x)\n",
    "    #     x = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    #\n",
    "    #     model = Model(inputs=x_input, outputs=x)\n",
    "    #\n",
    "    #     return model\n",
    "\n",
    "    def residual_block(self, x, filters, down_sample=None):\n",
    "        x_shortcut = x\n",
    "        if down_sample:\n",
    "            stride = 2  # The more stride, the smaller the picture\n",
    "            x_shortcut = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='valid')(x_shortcut)\n",
    "            x_shortcut = BatchNormalization()(x_shortcut)\n",
    "        else:\n",
    "            stride = 1\n",
    "\n",
    "        x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        x = Conv2D(filters, kernel_size=(3, 3), strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Add()([x_shortcut, x])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    # def ResNet50(self, ):\n",
    "    #     x_input = Input(INPUT_SHAPE)\n",
    "    #     x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(x_input)\n",
    "    #     x = BatchNormalization()(x)\n",
    "    #\n",
    "    #     x = Activation('relu')(x)\n",
    "    #     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    #\n",
    "    #     x = residual_block(x, filters=64)\n",
    "    #     x = residual_block(x, filters=64)\n",
    "    #     x = residual_block(x, filters=64)\n",
    "\n",
    "\n",
    "    #     x = residual_block(x, filters=128, down_sample=True)\n",
    "    #     x = residual_block(x, filters=128)\n",
    "    #     x = residual_block(x, filters=128)\n",
    "    #     x = residual_block(x, filters=128)\n",
    "    #\n",
    "    #     x = residual_block(x, filters=256, down_sample=True)\n",
    "    #     x = residual_block(x, filters=256)\n",
    "    #     x = residual_block(x, filters=256)\n",
    "    #     x = residual_block(x, filters=256)\n",
    "    #     x = residual_block(x, filters=256)\n",
    "    #     x = residual_block(x, filters=256)\n",
    "    #\n",
    "    #     x = residual_block(x, filters=512, down_sample=True)\n",
    "    #     x = residual_block(x, filters=512)\n",
    "    #     x = residual_block(x, filters=512)\n",
    "    #\n",
    "    #     x = GlobalAveragePooling2D()(x)\n",
    "    #     x = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    #\n",
    "    #     return Model(inputs=x_input, outputs=x)\n",
    "\n",
    "    def ResNet_18(self, ):\n",
    "        x_input = Input(INPUT_SHAPE)\n",
    "        x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding=\"same\")(x_input)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        x = self.residual_block(x, filters=64)\n",
    "        x = self.residual_block(x, filters=64)\n",
    "\n",
    "        x = self.residual_block(x, filters=128, down_sample=True)\n",
    "        x = self.residual_block(x, filters=128)\n",
    "        x = Dropout(0.5)\n",
    "\n",
    "        x = self.residual_block(x, filters=256, down_sample=True)\n",
    "        x = self.residual_block(x, filters=256)\n",
    "        x = Dropout(0.5)\n",
    "\n",
    "        x = self.residual_block(x, filters=512, down_sample=True)\n",
    "        x = self.residual_block(x, filters=512)\n",
    "        x = Dropout(0.5)\n",
    "\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=x_input, outputs=x)\n",
    "        return model\n",
    "\n",
    "    # def basic_model(self, ):\n",
    "    #     model = Sequential([\n",
    "    #         Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", input_shape=INPUT_SHAPE),\n",
    "    #         Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    #         MaxPool2D(pool_size=(2, 2)),\n",
    "    #         Dropout(0.25),\n",
    "    #\n",
    "    #         Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\", input_shape=INPUT_SHAPE),\n",
    "    #         Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    #         MaxPool2D(pool_size=(2, 2)),\n",
    "    #         Dropout(0.4),\n",
    "    #\n",
    "    #         Flatten(),\n",
    "    #         Dense(128, activation=\"relu\"),\n",
    "    #         Dropout(0.5),\n",
    "    #         Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "    #     ])\n",
    "    #     return model\n",
    "\n",
    "    def make_model(self, ):\n",
    "        model = self.ResNet_18()\n",
    "        # optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\n",
    "        model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        model.build(input_shape=(None, *INPUT_SHAPE))\n",
    "        return model\n",
    "\n",
    "    def splitting_dataset(self, features, target):\n",
    "        x_train, x_temp, y_train, y_temp = train_test_split(features, target, test_size=0.4, random_state=42)\n",
    "        x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "        return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "    def fitting_the_model(self, model, x_train, x_val, x_test, y_train, y_val, y_test):\n",
    "        learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                                    patience=3,\n",
    "                                                    verbose=1,\n",
    "                                                    factor=0.5,\n",
    "                                                    learning_rate=0.00001)\n",
    "\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range=0.1,  # Randomly zoom image\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False  # randomly flip images\n",
    "        )\n",
    "\n",
    "        datagen.fit(x_train)\n",
    "        print(np.min(x_train), np.max(x_train))\n",
    "        augmented_images, _ = next(datagen.flow(x_train, y_train, batch_size=10))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 10, figsize=(20, 20),\n",
    "                                 subplot_kw={'xticks': [], 'yticks': []},\n",
    "                                 gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "        # for i, ax in enumerate(axes.flat):\n",
    "        #     ax.imshow(np.clip(np.squeeze(augmented_images[i]), 0, 255), cmap='gray')\n",
    "\n",
    "        # plt.show()\n",
    "        print('Fitting Model ...')\n",
    "\n",
    "        # Fitting the model\n",
    "        epochs = 80\n",
    "        batch_size = 32\n",
    "        history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            verbose=1,\n",
    "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                            callbacks=[learning_rate_reduction])\n",
    "        self.plot_result(history)\n",
    "        model.save(\"model/dcnn\", save_format=\"tf\")\n",
    "        model.save(\"model/dcnn.h5\", save_format=\"h5\")\n",
    "\n",
    "        loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "        loss_v, accuracy_v = model.evaluate(x_val, y_val, verbose=1)\n",
    "        print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
    "        print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "        self.print_confusion_matrix(model, x_test, y_test)\n",
    "\n",
    "    def print_confusion_matrix(self, model, x_test, y_test):\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        fig, ax = plot_confusion_matrix(conf_mat=cm, show_absolute=True,\n",
    "                                        show_normed=True,\n",
    "                                        colorbar=True)\n",
    "\n",
    "        plt.savefig('./report/dcnn-confusion.jpg')\n",
    "\n",
    "        lesion_type = [\n",
    "            'Melanocytic nevi',\n",
    "            'Melanoma',\n",
    "            'Benign keratosis-like lesions ',\n",
    "            'Basal cell carcinoma',\n",
    "            'Actinic keratoses',\n",
    "            'Vascular lesions',\n",
    "            'Dermatofibroma'\n",
    "        ]\n",
    "        ax = sns.heatmap(cm, cmap=\"rocket_r\", fmt=\".01f\", annot_kws={'size': 16}, annot=True, square=True,\n",
    "                         xticklabels=lesion_type, yticklabels=lesion_type)  # What should I put here as a label\n",
    "        ax.set_ylabel('Actual', fontsize=20)\n",
    "        ax.set_xlabel('Predicted', fontsize=20)\n",
    "\n",
    "    def plot_result(self, history):\n",
    "        # Plot accuracy and loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label=\"Training accuracy\")\n",
    "        plt.plot(history.history['val_accuracy'], label=\"Validation accuracy\")\n",
    "\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label=\"Training loss\")\n",
    "        plt.plot(history.history['val_loss'], label=\"Validation loss\")\n",
    "\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(\"Accuracy and Loss graph Perceptron.png\")\n",
    "        plt.show()\n",
    "\n",
    "    def main(self):\n",
    "        print('Reading Data')\n",
    "        skin_df = self.read_data()\n",
    "        print('Preprocess Data')\n",
    "        skin_df = self.preprocess_data(skin_df)\n",
    "\n",
    "        features = skin_df[\"image\"]\n",
    "        target = skin_df[\"cell_type_idx\"]\n",
    "        print('Image processing')\n",
    "        features, target = self.image_preprocessing(features, target)\n",
    "        print('Image Splitting Dataset')\n",
    "        x_train, x_val, x_test, y_train, y_val, y_test = self.splitting_dataset(features, target)\n",
    "\n",
    "        print(features[:5])\n",
    "        print('Making model')\n",
    "        self.model = self.make_model()\n",
    "        self.fitting_the_model(self.model, x_train, x_val, x_test, y_train, y_val, y_test)\n",
    "\n",
    "    # ----------------\n",
    "    # LOAD MODEL\n",
    "    # ----------------\n",
    "    def load_model(self):\n",
    "        self.model = load_model(self.path)\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        input_image = self.preprocess_input_image(image_path)\n",
    "        input_image = self.datagen_input_image(input_image)\n",
    "        predict_probabilities = self.model.predict(np.expand_dims(input_image, axis=0), verbose=1)\n",
    "        result = np.argmax(predict_probabilities)\n",
    "        class_idx_to_label = {\n",
    "            0: 'Melanocytic nevi',\n",
    "            1: 'Melanoma',\n",
    "            2: 'Benign keratosis-like lesions',\n",
    "            3: 'Basal cell carcinoma',\n",
    "            4: 'Actinic keratoses',\n",
    "            5: 'Vascular lesions',\n",
    "            6: 'Dermatofibroma',\n",
    "            7: 'Clear Skin'\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'predict': predict_probabilities.tolist()[0],\n",
    "            'value': float(result),\n",
    "            'result': class_idx_to_label[result]\n",
    "        }\n",
    "\n",
    "    def get_external_image(self, url):\n",
    "        response = requests.get(url)\n",
    "        return response.content\n",
    "\n",
    "    def preprocess_input_image(self, image_path):\n",
    "        img = Image.open(BytesIO(self.get_external_image(image_path)))\n",
    "        # img = Image.open(image_path)\n",
    "        img = img.resize((100, 75))\n",
    "        img = np.asarray(img)  # Convert image to numpy array\n",
    "        img = (img - self.mean) / self.std  # Normalize the image\n",
    "\n",
    "        # # Blue starts here\n",
    "        # plt.imshow((img).astype('uint8'))\n",
    "        # plt.title(\"Normalized Image\")\n",
    "        # plt.show()\n",
    "\n",
    "        img = img.reshape(1, 75, 100, 3)  # Reshape to match model's input shape\n",
    "        return img\n",
    "\n",
    "    def datagen_input_image(self, input_image):\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "        )\n",
    "\n",
    "        datagen.fit(input_image)\n",
    "        augmented_iterator = datagen.flow(input_image, batch_size=1)\n",
    "        augmented_image = next(augmented_iterator)[0]\n",
    "        # plt.imshow(np.clip(np.squeeze(augmented_image), 0, 255), cmap='gray')\n",
    "        # plt.show()\n",
    "\n",
    "        return augmented_image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
